Transcript with 1-minute Timeline Markers
=====================================


[0:00:00]
what's up folks. In this video, we're going to talk about building agent workflows in eight levels of complexity. This is a follow up to one of my best videos from my channel, called building agents in three levels of complexity. And today, we're going to talk about how to go from a simple LMM call, all the way up to an agent workflow that uses an agent as a step. We're going to talk through the papers and the a little bit of the theory in a lot of the practice, right? Everything is going to be from scratch. Meaning we are they're going to use the basic and lame APIs, or we're just going to build stuff ourselves in pure Python. All right. So we're going to start easy, we're going to set up our environment variable for open API key. And we're going to run this. And now we're going to basically just write a simple LMM call. And that's super easy, right? We just have to import the open API, set up our client. And then we're going to write a little function. And I'm going to name the functions like level one, level two, level three to like organize our thoughts. And the first one's called level one. And it's the
[0:01:00]

[0:01:00]
basic LMM call, which essentially just calls the responses API from open AI, uses the GPT five mini model, sends a simple prompt and gets a response back. Pretty basic. So now we're going to call it like what is an agent workflow. And when we run it, we'll take a few seconds. And we'll just provide us with the response, pretty basic simple stuff. We specify the model, the input, and that's it. Right, we can use this to do things like summarize, extract a little bit of information. And we're limited by the prompting mechanics of the specific API. So we're just going to prompt and give responses back. So it's text goes in, text comes out. Okay, great. However, now when we go to level two, now we can start thinking about how can we chain these LMM costs to make something a little bit more powerful, right? And a very cool resource for this topic is
[0:02:00]

[0:02:00]
building effective agents by anthropic, where they talk about workflow agents went to use them, how they work, and they go from scratch. And this article definitely inspired my desire to do this video that I'm doing right now. So go check it out. All right, so chaining LMM calls is super easy. All we got to do is essentially what we're going to do is we're going to set up a function that does the LLM call, so takes in some input and provides some output. And this calls the opening API like we did before, use the GPT five mini model and sends a prompt and gets a response back easy. However, the level true is all about chaining as many LLM calls as we want. So for that, what we're going to do is we're going to take we're going to start with an input in a list of prompts that we want to chain together. Then what we're going to do is we're going to loop over these prompts. And for each prompt, we're going to take a
[0:03:00]

[0:03:00]
result of calling the making a call to the LLM call function with that prompt on and save the result as an input for the next call. So that means that we start with the result as the input, right? And then we call the first prompt. And then the next one is going to take in the result of the previous one as the input for the next. And we're going to do that for as many prompts as we have on the list of prompts that we're going to give this function as an input. And then we're going to print that out. And that's pretty much what it is. And for the example usage, I'm putting here an input topic like a Gentic workflow. And then the prompt is first, we're going to create a three bullet point essay plan for this topic. And then we're going to write the essay following the plan strictly. All right. And when we call this, right, so I can call it with the input and the list of prompts, this is what happens. So what we're going to see is that first, we're going to see the
[0:04:00]

[0:04:00]
output of the first call to the LLM, which essentially is going to create a plan for the essay. And then the second one is going to write the essay following the structure from the previous output. So the essay that we're going to write now is going to be based on this essay plan. So it's going to be like defining the scope and then arguing the benefits and then examining the challenges. So that's pretty much all it's going to do. So we're going to wait a few seconds. I'm probably going to skip this. Perfect. So now we can see that the first output was the plan and the second output was the actual essay. So our chaining works. And we can do this for all sorts of lists of prompts for whatever workflow we're trying to build. All right. Now, number three starts to get interesting because we start
[0:05:00]

[0:05:00]
routing at a LEM calls. So routing essentially means this. If we go back to the building effective agents and we go to the router pattern section. Router here, we go here, we get augmented chaining, there you go, routing. So here we have an LLM first. And what the LLM is going to do is going to route the input to one of as many as we want LLMs, not as many as we want, but like to one of X LLMs. And one of those LLMs is going to take in the input. It's going to provide us with an output as simple as that. So to do that, all we got to do is we set up a little dictionary that's going to contain a tag that indicates what is the selected prompt for the LLM that has been routed. And in this case, I
[0:06:00]

[0:06:00]
have two simple tags, two simple classifications, right? We have a coding prompt and a math prompt. So if the router identifies that the input is related to coding, the router is going to output something that can be parsed as coding. And then we're going to, the prompt is going to be take this coding problem and produce a solution invite. So essentially, if the input is a coding problem or something related to producing code, the the router is going to output coding. And if the problems that are math, the router is going to output math. That's pretty much all it does. Obviously, it could be a little bit smarter input here, something like if none of these topics are like, if the input is not about any of these topics, we can produce an output like default, like none or something like that, but we don't really need to do that. So now level three, which is routing is going to take in some input in this dictionary containing the coding and the math prompts. And now
[0:07:00]

[0:07:00]
we're going to write a little selector prompt, which is going to say, analyze the input and classify it as one of the following categories. Then we're going to list the router keys, right, the keys from this dictionary. So coding and math. And then we give an example just to make sure that the model is going to get it right. Okay. So we show it like input, write a Python function to read a file and then output coding, input, salty equation, two x plus three equal 11. And then the output is pretty simple, right? And then we start with the input given by the user. Now what we do is we make a call to the other lab, which hopefully will return either coding or math. And then we print, we print that out so that we can make sure that everything's working. And then we identify the selected prompt from the dictionary based on the key that's going to come out of this LLM call. And finally, we make the LLM call and return
[0:08:00]

[0:08:00]
that. So the output of the LLM call with the selected prompt from this dictionary, which is going to be either the value for this coding prompt or the math prompt. And the input given by the user has the input that's going to be processed by this prompt with this LLM call, which is the call to this function that we wrote back here, called LLM call, which uses the GPT five mini model to process the output process the input. So when we do that, so when we do that, no, let me just go here, running the following prompt. So we're going to run that. And now we're going to use it. So I'm going to say write some code, they can load a PDF and extract the headers of the document. And then I give the dictionary that contains the possible routes. Okay. And we're going to print that output. So now we're going to send that out.
[0:09:00]

[0:09:00]
All right, perfect. So we're done running. So as you can see, the output was coding. And then it ran the output for the coding prompt. And it produced some very reliable output, produced something that makes sense with the prompt. So we're done. This is pretty good. Now, it's important to note that these two examples for chaining and routing, I adapted them from a really cool cool book from a cookbook from anthropic, which you can check out on this article as well. Let me just
[0:10:00]

[0:10:00]
show you cookbook. There you go. So if you're going here, so this is the cookbook. And I have it referenced in the notebook. And I'm going to make the notebook available on GitHub. So you you have access to it. And this example was from the basic workflow notebook. However, they did this tailor to the cloud API. And I did it tailored to the opening API. But it's pretty similar with the difference that they did some XML parsing. And I'm not doing that in the example we're seeing now. But this is an awesome notebook. If you want to do stuff from scratch, this is a great starting point. So going back, now we're going to go to level four, which is when we combine LLMs with the information of some available functions. And we put that information inside of the prompt to the model to see if the model is able to make calls to those functions. And the inspiration for that was from a paper called two former, where language models taught themselves how to use tools, a pretty simple way. All they did was to provide outputs as text, which
[0:11:00]

[0:11:00]
were inside of the text, they had special strings that could be parsed into function calls that we can run with Python code. Okay, like you can see here, this is this thing in purple can be run as a Python function, which calls this Q and API or Q and a function. Okay, so inspired by this paper, we're going to do a very simple example. So we're going to load up some imports. And what we're going to do here is essentially, we start with something very silly simple function, which essentially is going to set us up in the in a base directory. And then if it's not, it's going to handle some errors. The most important thing is that we're going to write two functions, one to read files and one to write files. So we're going to read a file. So this function is going to take in a file path. And it's going to set the base directory and then read the file on side of that
[0:12:00]

[0:12:00]
path. Same thing for the right file function, it's going to set the base directory and then it's going to write a file in the specified file path. Okay, pretty simple, pretty easy. So now, what we're going to do is the level four is going to be about merging the LLM with the functions inside of the prompt. So for that, all we're going to do is we're going to write a little piece of code that gets the function as a string. So I use the inspect module, which I didn't know existed, honestly, up to today. So that was pretty cool. Because in the previous example that I did on my latest video, my lesson, my last video on this topic, I actually put the functions as strings inside of the prompt manually. So it's nice to know that I can do that programmatically. Now, I have a full prompt that contains the information about the functions to write files and read
[0:13:00]

[0:13:00]
files. Okay. And what we're going to do with that is we're going to send input to the OpenAI Responses API with the GPT five mini mod. And we're going to say you're a helpful assistant that can write and read files. This used to be the system message from the previous version of their API. But now they change it to instructions. It's the same thing. So now this model has the information about the two functions available plus has instructions about using these tools. So we're going to see if this model is able to produce an output that we can transform into an execution of the function indicated. So let's see if that's possible. So let's see. This call level for a Lamb function in prompt takes an input prompt like this, write a file called test the content hello world. And then I give it a list of the tools available. So write file and read file. Okay.
[0:14:00]

[0:14:00]
And what we're going to do now. Yeah, we're going to now we're going to run this. And we're going to see the output of the function call. We're going to see the output of this function. So as you can see, the output of this function is exactly what we need, which is a string that exactly reads as the calling of the right function for the task. So what we're going to do is we're going to use Python's built in exact module, which you can check out in the Python documentation, which essentially supports dynamic execution of Python code. Okay, so supports dynamic execution Python code. That's what it does. So we're going to use it to execute this string that indicates that we have to write the file. Okay, so when we do that, what happens is that we execute the action. So if we do a cat on the output of
[0:15:00]

[0:15:00]
test.txt to see if it was created, you see that it was created. So it's perfect. So what I want you to start seeing is that we're evolving from calling LLM APIs to trying to give LLM the ability to perform actions. But we are in the initial stages of that. Let's see how we can evolve even further. Now we're going to look at something called structured outputs. Now structure outputs, it's a very interesting thing. What it does is essentially give these LLM the ability to take unstructured data and produce structured data, so structured outputs. That's what it does. And there's no official paper on this topic, which I found kind of weird, but I think one of the best resources that you can see that are open source on the topic are either this JSON form or GitHub repo from hugging face, where they show a bulletproof way to generate structured
[0:16:00]

[0:16:00]
JSON from language models. It's super cool, and you can see a little bit about it. But the major source for knowledge on this topic has been the OpenAI Docs on structure outputs when they released it in 2024. And essentially what you see is that you can produce structured output. So an object given something called a JSON schema. And what the JSON schema is is essentially something that defines, okay, what is the, what is the structure of my object? Okay. So essentially, what that means is that we can indicate to the model, look, I want to produce an output that has this structure, and the model is going to produce an output that adheres to the structure that I want. So for example, I'm going to transform the previous example that we're doing into a structured output problem. So I'm going to create a class called right file operation. And I'm
[0:17:00]

[0:17:00]
going to be using a module, a very cool package in Python called by dentic. So what by dentic is by dentic. So by dentic is essentially a data validation library in Python that works amazingly well with LLM's. It's probably one of the most important packages out there on the LLM era that we're living right now. So what we're going to do is we're going to create a little object called right file operation. It's going to have two attributes, the file path, which contains the file where the file should be saved, and the contents of that file. So this is mimicking the operation of writing a file with the right file function. But instead of calling that function, we're producing the arguments to call that function. Hopefully, you see where we're going with this. And we're going to do the same with the reap file function. So reap file operation in the
[0:18:00]

[0:18:00]
file path. Now, we're going to write a prompt like write a file called test2.txt with the content hello world again. And we're going to make a call to the open AI responses API. But with the little added feature, we're going to set the best format for the output of this model to adhere to the right file operation object. So that means that this output is going to contain these attributes file path and content. Hopefully, being tailored to the prompt that we're writing right here. So the file path hopefully is going to be test2.txt. And the contents are going to be hello world again. So now we're going to do that. And then we're going to see what that output looks like. Perfect. So as you can see, we were able to obtain exactly what we wanted. So we obtained it programmatically, because we
[0:19:00]

[0:19:00]
created something that's a structured object represented by response dot output parts, which if we look at it is actually the object that we were hoping for. So if I create this here and I output, you can see that it says the object, it contains the things that we were looking for. So now we can write level five. And level five is going to be about leveraging this to execute an action, similarly to what we did before with the functions inside of the prompt. But this time we're leveraging structured outputs instead. So we're going to write the response to the LMBPI. And we're going to write the format to be right file operation. And then we're going to get the arguments back programmatically. And then we're going to call the function right file super simple, super easy. So now we're going to call
[0:20:00]

[0:20:00]
this and see what happens. Hopefully what's going to happen is that the model is going to extract this as file path. And this has the content of the file. And then it's going to execute the function correctly. Now, obviously, if I was doing something more serious, I would have to put a bunch of error handling here and a bunch of other stuff. But for now, I think that this is more than enough. So we're going to call this it seems like it worked. So we're going to test that out. Perfect it works. So now we're going to go even further. So we're going to go to level six and level six is function calling. Now, it's important to note that I'm talking about structured outputs before I'm talking about function calling, which is a choice I made. But chronologically, function calling came first, it
[0:21:00]

[0:21:00]
was released in June of 2023 in the open API. If we go to the release post, we can see here, June, June 2023, the function calling API was released by OpenAI. And if we go to the release post by structure outputs, that was in August of 2024. However, conceptually structure outputs underlies what makes function calling possible, because it's the ability to produce outputs that adhere to a certain JSON schema structure. So what is the function calling? We already saw that in practice, we saw that we can transform the outputs of an LLM into calls to functions. So function calling is just the integration of the capability directly into the LLM API, in this case, the opening API. And we're going to do that by leveraging what the open API allows us to do. And
[0:22:00]

[0:22:00]
we're going to make cost of functions, we're going to allow LLMs to take actions autonomously with this capability similarly to what we did before. So to do that, we're going to set up a better way to inform the LLM about the available tools or functions. And then we're going to see if the LLM is able to call them. So to do that, we got the final list of callable tools for the model. And that list is going to be a list of dictionaries that have a type, in this case, function, and then we say write file, we give a description, and we give the parameters that go with a function. So this is similar to put the to putting the functions inside of the prompt, but we do it as a JSON object to be a little bit more structurally sound. So we're doing that for both functions, we're defining the required parameters, the contents of each of the parameters
[0:23:00]

[0:23:00]
or if things good. So once we've done that, what we're going to do next is we're going to write down, I'm going to rewrite the read and write file functions just to be thorough. And what we're going to do now is we're going to set up a list of inputs through the model, which is just going to have one input. And the inputs can to be create a file called test function calling.txt with the content level six function calling. That's it. So now we're going to make a call to the open API. We're going to set the tools parameter to the list that we created called tools. And we're going to set two choice to auto, because that sets the choice of a to call to be automatically selected by the model, in this case, by GPT five mini. And then we're going to give the input as this little list of inputs, which could be just the just the prompt itself. It doesn't have to be this list of dictionaries. Okay, however, I think the responses API is kind of backwards compatible with the previous version. And
[0:24:00]

[0:24:00]
that's why this thing still works. So now we're going to do that. And then what we're going to do is we're going to save the functional call outputs for subsequent requests. So stick with me on this. So here's the thing, when we get the output from the model, we're going to inspect to see if any of the items inside of the output, which is going to be a list of objects, any of them should have a type of function call, which indicates that a call to a function was made. And then we're going to inspect the names of those function calls. And if they match the functions that we actually have, we are going to take care of the part of making of calling those functions. So we're going to say, okay, let's obtain the arguments that the model determine should be used for the function call. And then here, I'm doing it kind of like semi manually, because I already
[0:25:00]

[0:25:00]
know for the right file, what are the arguments? So I'm taking them here like this. And then I'm calling I'm making myself the call to the right file function. Now this is very similar to what we did before, but we're leveraging the API to be more deterministic about these executions of these functions. And they were appending the outputs of these executions, so that the model can compile the initial input and the outputs of the functions to produce a comprehensive response to the user. So we're doing a similar thing to read file. And then finally, we make a second call to the LLM containing everything that happened, including the initial prompt in the function calls and their outputs. So now we can inspect
[0:26:00]

[0:26:00]
what the final output looks like. So I'm going to run this. And there you go. It seems like the file was created. And we can test that and perfect. It was created. Awesome. So there you go. Now it starts to get interesting, because now we've learned about basic LLM calls, chaining calls, routing calls, putting functions inside prompts, using structured outputs to transform and structure data via LLM's into structured objects. And we've learned about function calling with the open API. Now, let's build a agent. Let's build something that can autonomously, autonomously make decisions and execute actions and loop over the usage of these tools and execution of
[0:27:00]

[0:27:00]
actions to perform complex tasks. So the inspiration for this next demonstration comes from a really awesome paper called React. And this paper, which I have it listed here in the notebook, which is called synergizing reasoning and acting language models. What this paper does is they show that there's a way that you can prompt the model to incentivize the interleaving of thoughts, like plans for what to do, and actions, which mean essentially executing some programs and function, which will lead to an observation that the model is going to use to be able to get further and further in solving the task. So this paper is really cool. And what you can do with this is what we're going to try to do is we're going to try to build from scratch only only using the LLM API in Python code. This entire react agent loop, which is a really good way to understand how modern agents work. So I'm going to, again,
[0:28:00]

[0:28:00]
write the read file, write file functions back into the cell here just to be thorough. Then I'm going to write a little mapping object here, which is essentially just dictionary that maps the names read file and write file to the correct corresponding functions. Okay. Now we're going to have a list of tools, which is the same as we did before. Okay, it's pretty similar to the thing that we did before. So it's the write file read file to descriptions as dictionaries. Okay, so these are the JSON schemas of those functions. Now, for the instructions for this model, we're going to have a few things that we need to address. The first one is this, your careful system that can use tools to read and write files. Okay, great. If a task involves files, call the appropriate tool with
[0:29:00]

[0:29:00]
precise arguments. After two calls, summarize results for the user, only write files when explicitly asked or when necessary to complete the task. This is just to increase the chance of a good two calls. So the model being precise about calling the right tools. And then if something is ambiguous, ask a concise clarifying question. When you're done, respond with final answer. And I'm doing this here just because we want to be able to parse some indication that a loop of actions should end. And we can actually provide an answer to the user. So now I wrote this with AI and just to safely load the JSON schemas of the tools that were going to be used. And then we're going to have something to if a loop goes for too long, we want to have something like this that summarizes what happened to the model. And without overloading the context
[0:30:00]

[0:30:00]
window of the model, so all the text the model can read. And now we're going to finally write level seven, the react agent loop. And what we're going to have for this function is a task prompt, which describes what the problem is. And then we're going to set the maximum number of turns that the model can actually loop and use tools to solve the task. Okay, I set the default to eight, but you can do it for like three. And that should be fine. Now we're going to start with that input. And here's what we're going to do. We're going to loop over the maximum amount of turns that we can do. And each turn involves making a call to the LLM API, reading and executing the functions that are called. And then seeing if we've reached the final answer, if not, we're going to go yet. So for that, we're going to start with calling the model using the similar structure that we did before. And now, so what we're going to be doing here is we're
[0:31:00]

[0:31:00]
going to be taking all the function calls that we identifying the model's response output. And we're going to execute those function calls, just like we did in the previous example. So we're going to identify them by the name. And then we're going to use the pi tools mapping dictionary that we built for the write file and read file. And then we're going to use that to get the actual function to be able to execute that function. So you execute the function, we write a little bit of some try accept statements to make sure that we can handle some error messages. And then we append the output of the execution of the functions to the input list that the model is going to be able to see at the end to be able to compile a proper response to the user. And what we do lastly is if and when the model produces the final answer text inside of the output, that means that we're
[0:32:00]

[0:32:00]
done, we don't have to loop anymore, and we can provide an answer to the user. So we parse that from the output using the rejects module. And then we return, and if we don't for some reason get to get to an actual output from the agent after the all of the terms that we set with the max_turns parameter, we just stop because we reach the maximum. All right. So now, we're going to set up a complex task that involves two actions, right, reading a file and then writing a summary of it to another file. And we're going to test it with our react agent loop. And we're going to see if it works. Perfect. Seems like it works. And we can test that by going to the summary file. We see that we got the summary file. And we see that it's talking about embeddings. If we go to the sample input file, we see that the input was about embeddings. So it seems like it works. So this
[0:33:00]

[0:33:00]
is great. What we're going to do now is we're going to go to the final level, let's say, which is the actual Agentec workflow. And this is going to be an Agentec workflow that uses agents. But instead of the end goal being the agent, the agent is just a step in a more complex system that merges the idea of simpler steps using just a simple LML call with possible agents, depending on the routing mechanics of the previous input. What I mean by that is we're going to take some input. And then we're going to use a router LML call like we did in level three. Remember, we did first test it for a LML call. And then we did a chaining of LML calls. And the third example was the routing procedure, where it takes an input and then it can route to one LML call or another. We're going to do that same thing here. But this time, since we've learned about structured outputs, we're going to be using structured outputs to do the routing mechanics. And what we're going to do next is we're going to
[0:34:00]

[0:34:00]
route to either a jokes agent, which is going to be some agent that can write to files and creates jokes or to a web search agent, which can search the web and also write to files. Then the output is going to be produced by either of those agents, depending on the input. That's what we're going to do. So to start things off, we're going to set up the routing mechanics. So to do that, we're going to set up a class called router output. And this class is going to have one attribute called category, which is going to be either the string joke or the web search. So it's a simple classification output. And then we're going to describe category of the input joke or web search. And then finally, we're going to make a test call to the LML to the OpenAI Responses API, setting the text format to the router output to see if this thing works. So I'm going to test that out right now. Perfect. It seems like it works. It was able to identify that the input was about creating jokes.
[0:35:00]

[0:35:00]
Pretty simple problem. And now what we're going to do is we're going to set up the tools for these agents. So for the Joker agent, it's just a simple write file tool. And for the web search agent is the write file tool and the ability to search the web, which in the OpenAI Responses API, it already has a built-in tool for a web search, which we're going to use. So that's what we have on the Joker agent tools, pretty similar to what we did before. But this time is just for the write file and web search, we have the write file tool. And we have this, which is the statement that indicates the OpenAI Responses API that the tool to use is the built-in web search tool. Then I, this is just because we're doing this from scratch. I did a little dictionary that if the routing call is joke, we're going to route it to the Joker agent with
[0:36:00]

[0:36:00]
these instructions and these tools. And if it's web search, we're going to route it to the web search agent with these instructions and these tools. Pretty simple, right? So now we're going to run that. All right. So after that, what we're going to do is we're going to set up the this tool map variable, which is just a dictionary that maps the write file to the function. So we run that. And now what we're going to do is we're going to define a react agent loop similar to the one that we did before, but with a catch. This is going to be a more modular function that allows us to create specialized agents. So it will take in the input. It will take in the agent dictionary, which is going to be either this or this, which essentially takes three keys, the name, the instructions and the tools and the max turns, just like we did before. And this is going to be very similar to before, but with the addition
[0:37:00]

[0:37:00]
that now we can actually make specialized agents based on the custom set of instructions and a list of tools, similar to what different frameworks do out there, where you can create specialized agents with very simple input. So we're going to set up our input list. We're going to set up the agent tools by extracting it from the agent dictionary. And then we're going to set up the loop similar to what we did before. This is going to loop is going to make a call to the open API with the instructions for the agent that will be used, selected by the router call. And then we're going to put that into the input list so that this so that information will be stored in the log of the agent. We're going to execute the function calls. Remember folks that if the agent uses the web search tool, this function call is this is not necessary because the web search
[0:38:00]

[0:38:00]
is executed by the opening API by default. So this is just for functions that are defined outside of the scope of the building tools. So then we're going to extract. We're going to map it exactly similar to what we did before. And the reason why I'm rewriting this is because I want to make each of these levels kind of like self-contained. Then we're appending the outputs of the function call execution of the function calls. And we're doing similar to what we did before. We're scraping the final answer. We're scraping the final answer with the rejects module and then appending everything and returning stopped if the maximum number of turns is reached without a conclusion. Okay, perfect. So this looks good. And now we can run this and we can finally write the function that represents level 8 which is the agentic workflow using this agent as a step. So
[0:39:00]

[0:39:00]
remember what we're building is a simple agentic workflow that takes first a router which is going to use an element to route the call to the input to either the jokes agent or the web search agent. And from there we're going to provide a final output. Super simple. So when we go back what we're going to do is we're going to start with the router. So we're going to obtain the classification either jokes or web search. Then we're going to, based on that, we're going to select the appropriate agent and the output is going to be the execution of that agent. That's it. Super simple. Super easy. And we were able to reproduce without having to use external framework or anything like that this agentic workflow using plain Python code and the LLM API. We're now using the OpenAI agents SDK. We're now using an agentic framework. We're building everything ourselves to understand
[0:40:00]

[0:40:00]
under the hood what's going on with these agents or agentic workflows. So now that we did that we can run this code. And now we're going to make some tests. So the first one is going to be about jokes. So pretty simple. Let's see if this works. We're going to run it. So this is just tell me a joke about AI. Perfect. It works perfectly. So as you can see it created a joke. Now let's look at something that involves writing the jokes but then also writing the file. So that means that two actions have to be executed. So let's see what happens here. All right, perfect. We can see that two iterations were needed for this agent to finish and it seemed to have created the the right file so we can take a look. Perfect. So it created five jokes, saved them to file. Beautiful. Final test we're going to do is the input for the web search agent. So we're going to research the latest
[0:41:00]

[0:41:00]
papers from 2025 about building agentic workflows. And we're going to save that to a file called agentic workflows 25 summary. So now we're going to test this input on our workflow and we're going to see if this works. All right, so the router output is correct. Web search in the selected agent was the web search agent so everything is working so far. All right, perfect. We're done running. It took like a minute 13 seconds but it seems like it searched the web and it saved to a file. So we're going to look for that file. Beautiful. Look at that. It looked, created a very nice report, has a bunch of papers. I'm pretty certain that everything worked out and if I wasn't I could inspect the intermediary outputs of this function. But for now that's it. So today in this video you've learned like eight levels of complexity to get to an
[0:42:00]

[0:42:00]
agentic workflow that uses an agent just as a step in a more complex hybrid workflow. We've talked about LM calling, chaining, routing, all the way up to agents and agentic workflows. We've tried to do everything kind of from scratch so that you understand the mechanics of function calling, structured outputs, routing patterns, and things like that. I think those topics are really important if you want to understand how to build off awesome agentic workflows. And on an upcoming video I want to tackle from scratch as well. Things like evaluation and making these systems work. And I'll see you there. Thanks for watching. Don't forget to like and subscribe and see you next time. Cheers.