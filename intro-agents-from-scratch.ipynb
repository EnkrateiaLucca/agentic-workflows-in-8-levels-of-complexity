{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53e47c17",
   "metadata": {},
   "source": [
    "# Introduction to Agentic Workflows from Scratch\n",
    "\n",
    "# 8 Levels of Agentic Workflows\n",
    "\n",
    "1. Simple LLM Call\n",
    "2. Chaining LLM calls\n",
    "3. Routing llm calls\n",
    "4. LLMs + functions in prompt\n",
    "5. LLMs + structured outputs\n",
    "6. LLMs + function calling\n",
    "7. Agent loop\n",
    "8. Agent as a step of an Agentic Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70af4dc8",
   "metadata": {},
   "source": [
    "# 1. Simple LLM Call\n",
    "\n",
    "- [OpenAI API Quickstart](https://platform.openai.com/docs/quickstart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d9efa450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"var: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a908a957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Short answer\\nAn agentic workflow is a way of structuring a process so autonomous software agents — AI components that can plan, take actions, use tools, monitor outcomes, and adapt — carry out most of the steps needed to achieve higher-level goals, with humans intervening for oversight, exceptions, or final approval.\\n\\nWhat it means in practice\\nInstead of a rigid, linear pipeline where each task is hand-coded, an agentic workflow gives an agent a goal and a set of capabilities (APIs, tools, knowledge, memory). The agent decomposes the goal into subtasks, chooses and calls tools, iterates based on feedback, and either completes the goal or escalates to a human. The agent is “agentic” because it acts autonomously and adapts rather than just executing a fixed sequence.\\n\\nCore components\\n- Goal/specification: the high-level objective the agent must achieve.\\n- Planner/Reasoner: decomposes goals into steps and generates plans.\\n- Tools/Actions: APIs, database queries, web scrapers, code execution, etc., the agent can call.\\n- Memory/State: short- and long-term context the agent can read/write.\\n- Executor/Orchestrator: runs the plan, handles retries and sequencing.\\n- Monitor/Verifier: checks results for correctness, consistency, and safety.\\n- Human-in-the-loop hooks & guardrails: approvals, constraints, and fallbacks.\\n\\nArchitectural patterns\\n- Single autonomous agent: one agent does planning and execution.\\n- Hierarchical agents: a high-level agent delegates to specialized sub-agents.\\n- Multi-agent collaboration: several agents with different roles (researcher, critic, executor).\\n- Reactive vs deliberative: fast, reactive agents for simple tasks; deliberative agents for complex planning.\\n- Blackboard or message-bus orchestration for complex systems.\\n\\nTypical workflow steps\\n1. Receive goal/input.\\n2. Interpret and clarify (ask questions if needed).\\n3. Plan/subtask decomposition.\\n4. Select and call appropriate tools for each subtask.\\n5. Validate intermediate outputs; loop if needed.\\n6. Integrate results and perform final verification.\\n7. Deliver outcome or escalate to human review.\\n\\nExample (customer support agentic workflow)\\n- Goal: resolve a customer refund request.\\n- Agent reads ticket, queries order DB, checks refund policy (tools).\\n- If eligible, initiates refund API call, notifies billing, drafts customer reply.\\n- Verifier checks refund amount and audit trail; human approves high-value refunds.\\n- Final message sent to customer.\\n\\nBenefits\\n- Handles open-ended, multi-step tasks that traditional pipelines struggle with.\\n- Scales decision-making and integrates diverse tools dynamically.\\n- Faster iteration and adaptation to changing conditions.\\n\\nRisks and mitigations\\n- Hallucinations/wrong actions: use verifiers, tool results as ground truth, and math/logic checks.\\n- Misaligned goals: define clear constraints, reward signals, guardrails.\\n- Security/privacy: limit tool access, sandbox execution, audit logs.\\n- Resource/cost blowup: set budgets, timeouts, and action limits.\\n\\nWhen to use one\\n- Tasks that require planning, tool use, and adaptation (research assistants, multi-step automations, orchestration across systems).\\n- Not ideal for trivial, deterministic pipelines where simpler automation suffices.\\n\\nQuick checklist to design an agentic workflow\\n- Define clear goals, success criteria, and constraints.\\n- Inventory available tools/APIs and permissions.\\n- Choose an agent architecture (single, hierarchical, multi-agent).\\n- Build monitoring, verification, and human escalation paths.\\n- Test on edge cases and instrument for auditability and cost control.\\n\\nIf you want, I can sketch a concrete agent design for a task you care about (e.g., content creation, customer ops, data extraction) with recommended tools, prompts, and safety checks.'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def level1_llm_call(prompt: str)->str:\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5-mini\",\n",
    "        input=prompt\n",
    "    )\n",
    "    \n",
    "    return response.output_text\n",
    "\n",
    "level1_llm_call(\"What is an agentic workflow?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ab2a2e",
   "metadata": {},
   "source": [
    "# 2. Chaining LLM Calls\n",
    "\n",
    "- https://www.anthropic.com/research/building-effective-agents\n",
    "- https://github.com/anthropics/anthropic-cookbook/blob/main/patterns/agents/basic_workflows.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "76805d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Define and scope \"agentic workflow\": introduce the concept as a workflow in which autonomous agents (software/AI or human+AI hybrids) pursue goals, make decisions, and coordinate actions within an environment; distinguish agentic from purely automated or scripted workflows; identify core components to analyze (agents, objectives, sensors/inputs, actuators/outputs, feedback loops, orchestration/coordination). Suggest concrete framing for the essay’s first section: formal definition, taxonomy (single vs. multi-agent; reactive vs. planning), and a brief illustrative example (e.g., an AI agent managing cloud resources or a multi-agent customer-support pipeline).\n",
      "\n",
      "- Argue the benefits and practical use cases: show how agentic workflows increase scalability, adaptability, and efficiency by enabling decentralized decision-making, continuous optimization, and context-aware interventions; discuss productivity and personalization gains (DevOps automation, autonomous data pipelines, robotic process automation, multi-agent conversational systems). Support with metrics/claims to include (reduced latency, fewer manual interventions, faster incident resolution) and one short case study to demonstrate real-world impact.\n",
      "\n",
      "- Examine challenges, risks, and governance needs, and propose mitigations: cover reliability, unintended behavior, coordination failures, transparency/explainability, accountability, privacy/security, and human oversight requirements; outline evaluation criteria (robustness, interpretability, performance, ethics) and governance measures (human-in-the-loop checkpoints, monitoring/rollback, auditable logs, formal specifications, regulatory compliance). End with a brief concluding tack: balancing autonomy and control to realize agentic workflows’ benefits responsibly.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1) Definition, scope, and core components\n",
      "\n",
      "- Formal definition\n",
      "  - An \"agentic workflow\" is a structured sequence of tasks and decisions carried out by one or more autonomous agents — either software/AI agents or human+AI hybrid agents — that pursue objectives, sense the environment, take actions, and coordinate to achieve goals in a production environment. Unlike purely automated or scripted workflows (which follow predetermined sequences and conditional branches), agentic workflows give agents authority to make decisions, plan, and adapt their behavior based on feedback and evolving context.\n",
      "\n",
      "- Distinguishing agentic from scripted automation\n",
      "  - Scripted automation: deterministic flows, explicit branching, human-specified rules; limited adaptivity.\n",
      "  - Agentic workflow: agents can reason, re-plan, trade off objectives, and coordinate dynamically; supports decentralized decision-making and emergent behaviors.\n",
      "\n",
      "- Core components to analyze\n",
      "  - Agents: autonomous decision-makers (software models, robotic controllers, or human+AI teams).\n",
      "  - Objectives (goals, utility functions, SLAs): what agents optimize or satisfy.\n",
      "  - Sensors / inputs: telemetry, logs, user queries, external APIs, business data.\n",
      "  - Actuators / outputs: API calls, configuration changes, messages to humans, physical actuations.\n",
      "  - Feedback loops: observation → evaluation → adaptation (monitoring, reward signals, metrics).\n",
      "  - Orchestration / coordination: protocols for interaction (queues, blackboards, negotiation, leader election), conflict resolution, and scheduling.\n",
      "\n",
      "- Taxonomy (concrete framing)\n",
      "  - By agent count:\n",
      "    - Single-agent: one decision-making entity controlling a workflow (e.g., an AI autoscaler that adjusts a cluster).\n",
      "    - Multi-agent: multiple agents with distributed responsibilities (e.g., a customer-support pipeline with distinct agents for triage, resolution, escalation).\n",
      "  - By behavior model:\n",
      "    - Reactive agents: map inputs to immediate actions (low-latency responses, fewer planning layers).\n",
      "    - Planning agents: build internal models and plan multi-step strategies (better for long-running, goal-directed tasks).\n",
      "  - Hybrids: combinations (a reactive front-end agent that routes to planning agents for complex cases).\n",
      "\n",
      "- Brief illustrative example\n",
      "  - Example: Multi-agent customer-support pipeline\n",
      "    - Agents: Intake bot (classifies ticket), Knowledge Retriever (fetches docs), Resolution Planner (plans steps), Human Escalation Agent (handles edge cases).\n",
      "    - Inputs: customer message, account telemetry.\n",
      "    - Outputs: automated replies, suggested fixes, tickets created, escalations.\n",
      "    - Coordination: the Intake bot routes; feedback loop monitors customer satisfaction and resolution time and retrains classifiers.\n",
      "\n",
      "2) Benefits and practical use cases\n",
      "\n",
      "- Core benefits\n",
      "  - Scalability: decentralizing decisions lets systems scale horizontally without linear increases in human oversight.\n",
      "  - Adaptability: agents continuously adjust to changing conditions (load, user behavior, data drift).\n",
      "  - Efficiency: reduce manual interventions, shorten decision latency, and automate continuous optimization.\n",
      "  - Personalization and context-awareness: agents can adapt actions per-user or per-context at scale.\n",
      "\n",
      "- Representative use cases\n",
      "  - DevOps automation / autonomous incident response: agents detect, triage, and remediate incidents.\n",
      "  - Autonomous data pipelines: agents monitor data quality, repair pipelines, and re-route workflows.\n",
      "  - Robotic process automation (RPA) with decision-making: agents handle exceptions and negotiate outcomes.\n",
      "  - Multi-agent conversational systems: modular bots that jointly handle complex dialogues (billing, technical support, upsell).\n",
      "  - Cloud resource management: agents optimize cost, performance, and SLAs by dynamic reconfiguration.\n",
      "\n",
      "- Metrics and claims to measure value (examples to report)\n",
      "  - Reduced latency: agentic routing can reduce response time (e.g., save 30–70% of decision latency vs. human routing).\n",
      "  - Fewer manual interventions: automating triage/repair can cut manual interventions by 50–90%, depending on domain.\n",
      "  - Faster incident resolution: mean time to resolution (MTTR) reductions of 30–60% in representative deployments.\n",
      "  - Resource efficiency: cost savings from autoscaling and optimization (10–40% in cloud spend in practice-case deployments).\n",
      "\n",
      "- Short case study (illustrative)\n",
      "  - Context: A mid-sized SaaS company deployed an agentic incident-response workflow. Agents continuously analyzed monitoring telemetry, performed root-cause classification, and executed safe remediation playbooks (rolling restarts, scaling, config rollbacks). Human operators retained override and audit capabilities.\n",
      "  - Results observed after three months (representative deployment):\n",
      "    - MTTR dropped from 90 minutes to 35 minutes (≈61% reduction).\n",
      "    - Manual interventions for recurring incidents dropped by 72%.\n",
      "    - Customer-impacting incidents per month decreased by 25% due to proactive remediation.\n",
      "  - Takeaway: decentralizing decision authority to agents for routine detection+repair improved responsiveness and freed engineers to focus on higher-value work. (Numbers are representative of a deployed pilot and will vary by environment; pilots should always include safety checks.)\n",
      "\n",
      "3) Challenges, risks, and governance needs, with mitigations\n",
      "\n",
      "- Key challenges and risks\n",
      "  - Reliability and brittleness: agents may fail under distributional shifts, ambiguous inputs, or adversarial conditions.\n",
      "  - Unintended behavior: agents optimizing proxy objectives can produce harmful side effects (reward hacking).\n",
      "  - Coordination failures: contention, oscillation, or deadlocks among multiple agents.\n",
      "  - Transparency and explainability: opaque decisions impede debugging, compliance, and trust.\n",
      "  - Accountability: unclear ownership when autonomous agents act autonomously.\n",
      "  - Privacy and security: agents processing sensitive data increase attack surface and exposure risk.\n",
      "  - Human oversight erosion: over-reliance on agents can degrade operator situational awareness.\n",
      "\n",
      "- Proposed mitigations (concrete)\n",
      "  - Reliability\n",
      "    - Robust training and validation on distributional variants; use domain randomization and adversarial testing.\n",
      "    - Canary deployments and staged rollouts; incremental authority increases.\n",
      "  - Preventing unintended behavior\n",
      "    - Formalize objectives and constraints; include negative constraints (safety rules) alongside rewards.\n",
      "    - Constrain action spaces and use conservative policies for high-risk operations.\n",
      "  - Coordination reliability\n",
      "    - Protocol design: use deterministic arbitration, backoff strategies, leader election, and timeouts.\n",
      "    - Simulation of multi-agent interactions before production release.\n",
      "  - Transparency and explainability\n",
      "    - Instrument actions with decision provenance, feature attributions, and human-readable explanations.\n",
      "    - Maintain dashboards that show rationale, confidence, and alternative actions.\n",
      "  - Accountability and human oversight\n",
      "    - Define responsibility matrices (who owns what decisions); require human-in-the-loop for high-impact actions.\n",
      "    - Implement approval gates for non-routine operations.\n",
      "  - Privacy/security\n",
      "    - Least-privilege access controls, encryption in transit/at rest, differential privacy where appropriate.\n",
      "    - Threat modeling and regular red-team exercises specific to agent behavior.\n",
      "  - Operational safeguards\n",
      "    - Monitoring and alerting for anomalies in agent behavior; automatic rollback triggers and safe-mode fallbacks.\n",
      "    - Immutable, auditable logs (see governance measures below).\n",
      "\n",
      "- Evaluation criteria (what to measure)\n",
      "  - Robustness: performance under distribution shift, fault injection outcomes, recovery latency.\n",
      "  - Interpretability: clarity of explanations, traceability of decisions to inputs and objectives.\n",
      "  - Performance: throughput, latency, MTTR, cost-efficiency, SLA attainment.\n",
      "  - Safety and ethics: frequency of constraint violations, privacy leakage metrics, fairness indicators.\n",
      "  - Human factors: operator workload, intervention frequency, trust and usability metrics.\n",
      "\n",
      "- Governance measures (practical controls)\n",
      "  - Human-in-the-loop checkpoints: require human approval for high-risk actions and periodic review of policies.\n",
      "  - Monitoring, alarms, and rollback: real-time telemetry, anomaly detectors, circuit breakers, and automated rollback paths.\n",
      "  - Auditable logs and provenance: immutable records of inputs, decisions, and actuations for forensic and compliance needs.\n",
      "  - Formal specifications and contracts: codified safety constraints, pre- and post-conditions for agent actions, and SLAs.\n",
      "  - Testing and verification: scenario-based testing, property-based testing, and formal verification where feasible.\n",
      "  - Organizational policies and compliance: role-based responsibilities, change-control processes, and regulatory alignment (e.g., data protection, sector-specific rules).\n",
      "  - Continuous governance loop: periodic audits, post-incident reviews, and updating of objectives/constraints based on lessons learned.\n",
      "\n",
      "4) Conclusion — balancing autonomy and control\n",
      "\n",
      "Agentic workflows can materially improve scalability, responsiveness, and personalization by delegating appropriate decision authority to autonomous agents. To realize benefits responsibly, organizations must pair autonomy with rigorous governance: clear objectives and constraints, robust testing and monitoring, auditable decision provenance, and human oversight for high-impact operations. The practical aim is not maximal autonomy but the right balance — letting agents handle routine, well-specified tasks while humans steer strategy, handle exceptions, and ensure ethical, reliable outcomes.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1) Definition, scope, and core components\\n\\n- Formal definition\\n  - An \"agentic workflow\" is a structured sequence of tasks and decisions carried out by one or more autonomous agents — either software/AI agents or human+AI hybrid agents — that pursue objectives, sense the environment, take actions, and coordinate to achieve goals in a production environment. Unlike purely automated or scripted workflows (which follow predetermined sequences and conditional branches), agentic workflows give agents authority to make decisions, plan, and adapt their behavior based on feedback and evolving context.\\n\\n- Distinguishing agentic from scripted automation\\n  - Scripted automation: deterministic flows, explicit branching, human-specified rules; limited adaptivity.\\n  - Agentic workflow: agents can reason, re-plan, trade off objectives, and coordinate dynamically; supports decentralized decision-making and emergent behaviors.\\n\\n- Core components to analyze\\n  - Agents: autonomous decision-makers (software models, robotic controllers, or human+AI teams).\\n  - Objectives (goals, utility functions, SLAs): what agents optimize or satisfy.\\n  - Sensors / inputs: telemetry, logs, user queries, external APIs, business data.\\n  - Actuators / outputs: API calls, configuration changes, messages to humans, physical actuations.\\n  - Feedback loops: observation → evaluation → adaptation (monitoring, reward signals, metrics).\\n  - Orchestration / coordination: protocols for interaction (queues, blackboards, negotiation, leader election), conflict resolution, and scheduling.\\n\\n- Taxonomy (concrete framing)\\n  - By agent count:\\n    - Single-agent: one decision-making entity controlling a workflow (e.g., an AI autoscaler that adjusts a cluster).\\n    - Multi-agent: multiple agents with distributed responsibilities (e.g., a customer-support pipeline with distinct agents for triage, resolution, escalation).\\n  - By behavior model:\\n    - Reactive agents: map inputs to immediate actions (low-latency responses, fewer planning layers).\\n    - Planning agents: build internal models and plan multi-step strategies (better for long-running, goal-directed tasks).\\n  - Hybrids: combinations (a reactive front-end agent that routes to planning agents for complex cases).\\n\\n- Brief illustrative example\\n  - Example: Multi-agent customer-support pipeline\\n    - Agents: Intake bot (classifies ticket), Knowledge Retriever (fetches docs), Resolution Planner (plans steps), Human Escalation Agent (handles edge cases).\\n    - Inputs: customer message, account telemetry.\\n    - Outputs: automated replies, suggested fixes, tickets created, escalations.\\n    - Coordination: the Intake bot routes; feedback loop monitors customer satisfaction and resolution time and retrains classifiers.\\n\\n2) Benefits and practical use cases\\n\\n- Core benefits\\n  - Scalability: decentralizing decisions lets systems scale horizontally without linear increases in human oversight.\\n  - Adaptability: agents continuously adjust to changing conditions (load, user behavior, data drift).\\n  - Efficiency: reduce manual interventions, shorten decision latency, and automate continuous optimization.\\n  - Personalization and context-awareness: agents can adapt actions per-user or per-context at scale.\\n\\n- Representative use cases\\n  - DevOps automation / autonomous incident response: agents detect, triage, and remediate incidents.\\n  - Autonomous data pipelines: agents monitor data quality, repair pipelines, and re-route workflows.\\n  - Robotic process automation (RPA) with decision-making: agents handle exceptions and negotiate outcomes.\\n  - Multi-agent conversational systems: modular bots that jointly handle complex dialogues (billing, technical support, upsell).\\n  - Cloud resource management: agents optimize cost, performance, and SLAs by dynamic reconfiguration.\\n\\n- Metrics and claims to measure value (examples to report)\\n  - Reduced latency: agentic routing can reduce response time (e.g., save 30–70% of decision latency vs. human routing).\\n  - Fewer manual interventions: automating triage/repair can cut manual interventions by 50–90%, depending on domain.\\n  - Faster incident resolution: mean time to resolution (MTTR) reductions of 30–60% in representative deployments.\\n  - Resource efficiency: cost savings from autoscaling and optimization (10–40% in cloud spend in practice-case deployments).\\n\\n- Short case study (illustrative)\\n  - Context: A mid-sized SaaS company deployed an agentic incident-response workflow. Agents continuously analyzed monitoring telemetry, performed root-cause classification, and executed safe remediation playbooks (rolling restarts, scaling, config rollbacks). Human operators retained override and audit capabilities.\\n  - Results observed after three months (representative deployment):\\n    - MTTR dropped from 90 minutes to 35 minutes (≈61% reduction).\\n    - Manual interventions for recurring incidents dropped by 72%.\\n    - Customer-impacting incidents per month decreased by 25% due to proactive remediation.\\n  - Takeaway: decentralizing decision authority to agents for routine detection+repair improved responsiveness and freed engineers to focus on higher-value work. (Numbers are representative of a deployed pilot and will vary by environment; pilots should always include safety checks.)\\n\\n3) Challenges, risks, and governance needs, with mitigations\\n\\n- Key challenges and risks\\n  - Reliability and brittleness: agents may fail under distributional shifts, ambiguous inputs, or adversarial conditions.\\n  - Unintended behavior: agents optimizing proxy objectives can produce harmful side effects (reward hacking).\\n  - Coordination failures: contention, oscillation, or deadlocks among multiple agents.\\n  - Transparency and explainability: opaque decisions impede debugging, compliance, and trust.\\n  - Accountability: unclear ownership when autonomous agents act autonomously.\\n  - Privacy and security: agents processing sensitive data increase attack surface and exposure risk.\\n  - Human oversight erosion: over-reliance on agents can degrade operator situational awareness.\\n\\n- Proposed mitigations (concrete)\\n  - Reliability\\n    - Robust training and validation on distributional variants; use domain randomization and adversarial testing.\\n    - Canary deployments and staged rollouts; incremental authority increases.\\n  - Preventing unintended behavior\\n    - Formalize objectives and constraints; include negative constraints (safety rules) alongside rewards.\\n    - Constrain action spaces and use conservative policies for high-risk operations.\\n  - Coordination reliability\\n    - Protocol design: use deterministic arbitration, backoff strategies, leader election, and timeouts.\\n    - Simulation of multi-agent interactions before production release.\\n  - Transparency and explainability\\n    - Instrument actions with decision provenance, feature attributions, and human-readable explanations.\\n    - Maintain dashboards that show rationale, confidence, and alternative actions.\\n  - Accountability and human oversight\\n    - Define responsibility matrices (who owns what decisions); require human-in-the-loop for high-impact actions.\\n    - Implement approval gates for non-routine operations.\\n  - Privacy/security\\n    - Least-privilege access controls, encryption in transit/at rest, differential privacy where appropriate.\\n    - Threat modeling and regular red-team exercises specific to agent behavior.\\n  - Operational safeguards\\n    - Monitoring and alerting for anomalies in agent behavior; automatic rollback triggers and safe-mode fallbacks.\\n    - Immutable, auditable logs (see governance measures below).\\n\\n- Evaluation criteria (what to measure)\\n  - Robustness: performance under distribution shift, fault injection outcomes, recovery latency.\\n  - Interpretability: clarity of explanations, traceability of decisions to inputs and objectives.\\n  - Performance: throughput, latency, MTTR, cost-efficiency, SLA attainment.\\n  - Safety and ethics: frequency of constraint violations, privacy leakage metrics, fairness indicators.\\n  - Human factors: operator workload, intervention frequency, trust and usability metrics.\\n\\n- Governance measures (practical controls)\\n  - Human-in-the-loop checkpoints: require human approval for high-risk actions and periodic review of policies.\\n  - Monitoring, alarms, and rollback: real-time telemetry, anomaly detectors, circuit breakers, and automated rollback paths.\\n  - Auditable logs and provenance: immutable records of inputs, decisions, and actuations for forensic and compliance needs.\\n  - Formal specifications and contracts: codified safety constraints, pre- and post-conditions for agent actions, and SLAs.\\n  - Testing and verification: scenario-based testing, property-based testing, and formal verification where feasible.\\n  - Organizational policies and compliance: role-based responsibilities, change-control processes, and regulatory alignment (e.g., data protection, sector-specific rules).\\n  - Continuous governance loop: periodic audits, post-incident reviews, and updating of objectives/constraints based on lessons learned.\\n\\n4) Conclusion — balancing autonomy and control\\n\\nAgentic workflows can materially improve scalability, responsiveness, and personalization by delegating appropriate decision authority to autonomous agents. To realize benefits responsibly, organizations must pair autonomy with rigorous governance: clear objectives and constraints, robust testing and monitoring, auditable decision provenance, and human oversight for high-impact operations. The practical aim is not maximal autonomy but the right balance — letting agents handle routine, well-specified tasks while humans steer strategy, handle exceptions, and ensure ethical, reliable outcomes.'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "def llm_call(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Call an LLM with a prompt.\n",
    "    \"\"\"\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5-mini\",\n",
    "        input=prompt\n",
    "    )\n",
    "    \n",
    "    return response.output_text\n",
    "\n",
    "def level2_chain_llm_calls(input: str, prompts: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Chain LLM calls together.\n",
    "    \"\"\"\n",
    "    result = input\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        result = llm_call(f\"{prompt}. Input: {result}\")\n",
    "        print(result)\n",
    "        print(\"-\" * 100)\n",
    "    \n",
    "    return result\n",
    "\n",
    "input_topic = \"Agentic Workflow\"\n",
    "prompts = [\n",
    "    \"Create a 3 bullet point essay plan for this topic\",\n",
    "    \"Write this essay following the plan strictly\",\n",
    "]\n",
    "level2_chain_llm_calls(input_topic, prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2951938a",
   "metadata": {},
   "source": [
    "# 3. Routing LLM Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0c2cc488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "routes = {\n",
    "    \"coding\": \"\"\"Take this coding problem and produce a solution in Python, \n",
    "    your output should be only the Python code and ONLY THAT.\n",
    "    Here is the coding problem:\n",
    "    \"\"\",\n",
    "    \"math\": \"\"\"\n",
    "    Take this math problem and produce a solution for it, \n",
    "    Make sure to organize it nicely as an explanatory markdown tutorial,\n",
    "    mixing markdown style text and latex for the math equations.\n",
    "    Here is the math problem:\n",
    "    \"\"\",\n",
    "}\n",
    "\n",
    "def level3_routing_llm_calls(input: str, routes: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Route the input to the appropriate route based on the input.\n",
    "    \"\"\"\n",
    "    selector_prompt = f\"\"\"\n",
    "    Analyze the input and classify it as one of the following categories: {list(routes.keys())}\n",
    "    Your output should ONLY be the category name and absolutely nothing else.\n",
    "    For example:\n",
    "    Input: 'Write a Python function to read a file'\n",
    "    Output: 'coding'\n",
    "    \n",
    "    Input: 'Solve the equation 2x + 3 = 11'\n",
    "    Output: 'math'\n",
    "    \n",
    "    Input: {input}\"\"\".strip()\n",
    "    \n",
    "    selector_response = llm_call(selector_prompt)\n",
    "    print(f\"Router output: {selector_response}\")\n",
    "    selected_prompt = routes[selector_response]\n",
    "    print(f\"Running the following prompt: {selected_prompt}\")\n",
    "    print(\"-\" * 100)\n",
    "    return llm_call(f\"{selected_prompt}. Input: {input}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1c4a5d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Router output: coding\n",
      "Running the following prompt: Take this coding problem and produce a solution in Python, \n",
      "    your output should be only the Python code and ONLY THAT.\n",
      "    Here is the coding problem:\n",
      "    \n",
      "----------------------------------------------------------------------------------------------------\n",
      "#!/usr/bin/env python3\n",
      "\"\"\"\n",
      "Extract repeated page headers from a PDF.\n",
      "\n",
      "Usage:\n",
      "    python extract_pdf_headers.py /path/to/file.pdf\n",
      "\n",
      "Requirements:\n",
      "    pip install PyMuPDF\n",
      "\"\"\"\n",
      "import sys\n",
      "import json\n",
      "import re\n",
      "from collections import defaultdict\n",
      "try:\n",
      "    import fitz  # PyMuPDF\n",
      "except ImportError:\n",
      "    sys.exit(\"PyMuPDF not installed. Install with: pip install PyMuPDF\")\n",
      "\n",
      "def normalize_text(s):\n",
      "    s = s.strip()\n",
      "    s = re.sub(r'\\s+', ' ', s)\n",
      "    # Remove common page number patterns like \"Page 1\", \"Pg. 1\", \"- 1 -\"\n",
      "    s = re.sub(r'^(page|pg|p)\\.?\\s*\\d+\\b[:.\\-–—]*\\s*', '', s, flags=re.I)\n",
      "    s = re.sub(r'\\b(page|pg|p)\\.?\\s*\\d+\\b[:.\\-–—]*\\s*$', '', s, flags=re.I)\n",
      "    s = re.sub(r'^[\\-\\u2013\\u2014\\.\\|]+\\s*', '', s)  # leading separators\n",
      "    s = re.sub(r'\\s*[\\-\\u2013\\u2014\\.\\|]+$', '', s)  # trailing separators\n",
      "    return s.strip()\n",
      "\n",
      "def extract_headers(pdf_path, top_fraction=0.12, top_points=80, min_repeat_ratio=0.3, min_repeat_pages=2):\n",
      "    doc = fitz.open(pdf_path)\n",
      "    total_pages = len(doc)\n",
      "    # threshold in points (pdf units) for \"top\" region\n",
      "    headers_by_page = defaultdict(list)  # page_idx -> list of header line strings\n",
      "    for i in range(total_pages):\n",
      "        page = doc[i]\n",
      "        page_h = page.rect.height\n",
      "        threshold = min(top_points, page_h * top_fraction)\n",
      "        page_dict = page.get_text(\"dict\")\n",
      "        for block in page_dict.get(\"blocks\", []):\n",
      "            if block.get(\"type\", 0) != 0:\n",
      "                continue\n",
      "            for line in block.get(\"lines\", []):\n",
      "                spans = line.get(\"spans\", [])\n",
      "                if not spans:\n",
      "                    continue\n",
      "                # Determine top Y coordinate of the line (min of spans' y0)\n",
      "                y0 = min(span.get(\"bbox\", [0,0,0,0])[1] for span in spans)\n",
      "                if y0 <= threshold:\n",
      "                    line_text = \"\".join(span.get(\"text\", \"\") for span in spans).strip()\n",
      "                    if line_text:\n",
      "                        headers_by_page[i].append(line_text)\n",
      "    # Flatten and count unique occurrences per page (dedupe same lines on same page)\n",
      "    candidate_map = defaultdict(set)  # normalized_text -> set(page_indexes)\n",
      "    for p, lines in headers_by_page.items():\n",
      "        seen_norm = set()\n",
      "        for ln in lines:\n",
      "            norm = normalize_text(ln)\n",
      "            if not norm:\n",
      "                continue\n",
      "            if len(norm) < 3:\n",
      "                continue\n",
      "            if norm.isdigit():\n",
      "                continue\n",
      "            if norm in seen_norm:\n",
      "                continue\n",
      "            seen_norm.add(norm)\n",
      "            candidate_map[norm].add(p + 1)  # use 1-based page numbers\n",
      "    # Decide which candidates are headers: appear on at least min_repeat_pages or min_repeat_ratio\n",
      "    results = []\n",
      "    min_by_ratio = max(min_repeat_pages, int(total_pages * min_repeat_ratio))\n",
      "    for text, pages in sorted(candidate_map.items(), key=lambda kv: (-len(kv[1]), kv[0])):\n",
      "        count = len(pages)\n",
      "        if count >= min_by_ratio:\n",
      "            results.append({\n",
      "                \"text\": text,\n",
      "                \"count\": count,\n",
      "                \"pages\": sorted(list(pages))\n",
      "            })\n",
      "    return {\"file\": pdf_path, \"total_pages\": total_pages, \"threshold_points\": None, \"headers\": results}\n",
      "\n",
      "def main():\n",
      "    if len(sys.argv) < 2:\n",
      "        print(\"Usage: python extract_pdf_headers.py /path/to/file.pdf\")\n",
      "        sys.exit(1)\n",
      "    pdf_path = sys.argv[1]\n",
      "    out = extract_headers(pdf_path)\n",
      "    print(json.dumps(out, indent=2, ensure_ascii=False))\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n"
     ]
    }
   ],
   "source": [
    "output = level3_routing_llm_calls(\"Write some code that can load a pdf and extract the headers from the document.\", routes)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d73b156",
   "metadata": {},
   "source": [
    "# 4. LLMs + functions in prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c86f2d",
   "metadata": {},
   "source": [
    "<img src=\"./2025-08-29-23-58-22.png\" width=50%>\n",
    "\n",
    "[Source](https://arxiv.org/pdf/2302.04761)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8d35ca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade openai\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Callable\n",
    "import inspect\n",
    "\n",
    "# --- Safe file helpers (restrict to a base directory) ---\n",
    "BASE_DIR = Path.cwd()  # change if you want a sandbox like Path(\"./sandbox\").resolve()\n",
    "\n",
    "def _ensure_under_base(p: Path) -> Path:\n",
    "    p = (BASE_DIR / p).resolve()\n",
    "    if not str(p).startswith(str(BASE_DIR)):\n",
    "        raise ValueError(\"Path escapes base directory\")\n",
    "    return p\n",
    "\n",
    "def read_file(file_path: str) -> str:\n",
    "    \"\"\"Read UTF-8 text from a file under BASE_DIR.\"\"\"\n",
    "    p = _ensure_under_base(Path(file_path))\n",
    "    return p.read_text(encoding=\"utf-8\")\n",
    "\n",
    "def write_file(file_path: str, content: str) -> str:\n",
    "    \"\"\"Write UTF-8 text to a file under BASE_DIR (create parents).\"\"\"\n",
    "    p = _ensure_under_base(Path(file_path))\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    p.write_text(content, encoding=\"utf-8\")\n",
    "    return f\"Wrote {len(content)} chars to {p.relative_to(BASE_DIR)}\"\n",
    "\n",
    "\n",
    "def level4_llm_and_functions_in_prompt(prompt: str, functions: List[Callable]):\n",
    "\n",
    "    # This line gets the source code of the functions as plain strings\n",
    "    function_info = \" \".join([inspect.getsource(func) for func in functions])\n",
    "    full_prompt_with_function_info = f\"\"\"\n",
    "    Take this request from a user: {prompt}.\n",
    "    If the request involves writing to a file or reading to a file,\n",
    "    you can output a call to these functions which you have access to: \n",
    "    {function_info}.\n",
    "    \"\"\"\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5-mini\",\n",
    "        input=full_prompt_with_function_info,\n",
    "        instructions=\"You are a helpful assistant that can write and read files.\"\n",
    "    )\n",
    "    \n",
    "    return response.output_text\n",
    "    \n",
    "output_function_call = level4_llm_and_functions_in_prompt(\"Write a file called 'test.txt' with the content 'Hello, world!'\", [write_file, read_file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "240e6b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write_file(\"test.txt\", \"Hello, world!\")\n"
     ]
    }
   ],
   "source": [
    "print(output_function_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9765c01",
   "metadata": {},
   "source": [
    "Now, to execute the functions we can use Python's built-in ['exec'](https://docs.python.org/3/library/functions.html#exec:~:text=This%20function%20supports,is%20None.) module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1a3e8429",
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(output_function_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0c86b58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!"
     ]
    }
   ],
   "source": [
    "!cat test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b9ab54",
   "metadata": {},
   "source": [
    "# 5. LLMs + Structured Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec87594",
   "metadata": {},
   "source": [
    "- [OpenAI API docs structured outputs](https://platform.openai.com/docs/guides/structured-outputs)\n",
    "- [JSON Schemas](https://json-schema.org/overview/what-is-jsonschema)\n",
    "- [Jsonformer: A Bulletproof Way to Generate Structured JSON from Language Models.](https://github.com/1rgs/jsonformer)\n",
    "\n",
    "There is evidence that the ability to produce structured outputs reliably is the result of:\n",
    "- Specialized fine-tuning (Toolformer, OpenAI function-calling, Claude JSON mode) on tool-use / schema-call datasets.\n",
    "- Decoding constraints (Jsonformer, constrained sampling, grammar-based decoding) to enforce syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f02c8a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test2.txt\n",
      "Hello, world Again!\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class WriteFileOperation(BaseModel):\n",
    "    file_path: str = Field(description=\"The path to the file to be written to or read from\")\n",
    "    content: str = Field(description=\"The content to be written to the file\")\n",
    "\n",
    "class ReadFileOperation(BaseModel):\n",
    "    file_path: str = Field(description=\"The path to the file to be written to or read from\")\n",
    "\n",
    "prompt = \"Write a file called 'test2.txt' with the content 'Hello, world Again!'\"\n",
    "\n",
    "response = client.responses.parse(\n",
    "    model=\"gpt-5-mini\",\n",
    "    input=prompt,\n",
    "    instructions=\"You are a helpful assistant that can write and read files.\",\n",
    "    text_format=WriteFileOperation)\n",
    "\n",
    "output_write_file_ops = response.output_parsed\n",
    "print(output_write_file_ops.file_path)\n",
    "print(output_write_file_ops.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "639ea659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WriteFileOperation(file_path='test2.txt', content='Hello, world Again!')"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.output_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "54cb1b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File was created!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WriteFileOperation(file_path='level5-structured-outputs.txt', content='Level 5: Structured outputs!')"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def level5_llm_structured(prompt):\n",
    "    response = client.responses.parse(\n",
    "        model=\"gpt-5-mini\",\n",
    "        input=prompt,\n",
    "        instructions=\"You are a helpful assistant that can write and read files.\",\n",
    "        text_format=WriteFileOperation # Structured OUTPUT from the LLM!\n",
    "        )\n",
    "    \n",
    "    output_args = response.output_parsed\n",
    "    # Calling the function with the arguments obtained using structured outputs from the LLM.\n",
    "    write_file(output_args.file_path, output_args.content)\n",
    "    print(\"File was created!\")\n",
    "    return output_args\n",
    "\n",
    "level5_llm_structured(\"Write a file called 'level5-structured-outputs.txt' with the content 'Level 5: Structured outputs!'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "543b9651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 5: Structured outputs!"
     ]
    }
   ],
   "source": [
    "!cat level5-structured-outputs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c3990b",
   "metadata": {},
   "source": [
    "# Level 6 - Function Calling\n",
    "\n",
    "- https://platform.openai.com/docs/guides/function-calling\n",
    "\n",
    "After a quick ChatGPT + Search chat, this is my current model of the relationship between structured outputs and function calling:\n",
    "\n",
    "- [Function calling came first](https://openai.com/index/function-calling-and-other-api-updates/) (as a user-facing API feature).\n",
    "- Structured outputs is the generalization of the same mechanism:\n",
    "  - Instead of only producing function arguments, the model can produce any schema-constrained JSON.\n",
    "\n",
    "Structured outputs as a technique is the foundation that makes function calling possible — but historically, the narrower [“function calling” was released first (as an API facing feature by OpenAI)](https://openai.com/index/function-calling-and-other-api-updates/).\n",
    "\n",
    "\n",
    "- Conceptually: Structured outputs (schema adherence) underlie function calling.\n",
    "- Chronologically: [Function calling was released first (June 2023)](https://openai.com/index/function-calling-and-other-api-updates/), [structured outputs generalized it later (2024)](https://openai.com/index/introducing-structured-outputs-in-the-api/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "642a0c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final input:\n",
      "{\n",
      "  \"id\": \"resp_68b270c1441c81a1b86d32b2293d2a030751cf3d6695eb09\",\n",
      "  \"created_at\": 1756524737.0,\n",
      "  \"error\": null,\n",
      "  \"incomplete_details\": null,\n",
      "  \"instructions\": \"Answer the user's question leveraging the information obtained from the usage of the tools.\",\n",
      "  \"metadata\": {},\n",
      "  \"model\": \"gpt-5-mini-2025-08-07\",\n",
      "  \"object\": \"response\",\n",
      "  \"output\": [\n",
      "    {\n",
      "      \"id\": \"msg_68b270c20cf081a1b710c059788ed1860751cf3d6695eb09\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"annotations\": [],\n",
      "          \"text\": \"Created file 'test-function-calling.txt' with the requested content.\",\n",
      "          \"type\": \"output_text\",\n",
      "          \"logprobs\": []\n",
      "        }\n",
      "      ],\n",
      "      \"role\": \"assistant\",\n",
      "      \"status\": \"completed\",\n",
      "      \"type\": \"message\"\n",
      "    }\n",
      "  ],\n",
      "  \"parallel_tool_calls\": true,\n",
      "  \"temperature\": 1.0,\n",
      "  \"tool_choice\": \"auto\",\n",
      "  \"tools\": [\n",
      "    {\n",
      "      \"name\": \"write_file\",\n",
      "      \"parameters\": {\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {\n",
      "          \"file_path\": {\n",
      "            \"type\": \"string\",\n",
      "            \"description\": \"The path to the file to write to.\"\n",
      "          },\n",
      "          \"content\": {\n",
      "            \"type\": \"string\",\n",
      "            \"description\": \"The content to write to the file.\"\n",
      "          }\n",
      "        },\n",
      "        \"required\": [\n",
      "          \"file_path\",\n",
      "          \"content\"\n",
      "        ],\n",
      "        \"additionalProperties\": false\n",
      "      },\n",
      "      \"strict\": true,\n",
      "      \"type\": \"function\",\n",
      "      \"description\": \"Write a file with the given content.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"read_file\",\n",
      "      \"parameters\": {\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {\n",
      "          \"file_path\": {\n",
      "            \"type\": \"string\",\n",
      "            \"description\": \"The path to the file to read.\"\n",
      "          }\n",
      "        },\n",
      "        \"required\": [\n",
      "          \"file_path\"\n",
      "        ],\n",
      "        \"additionalProperties\": false\n",
      "      },\n",
      "      \"strict\": true,\n",
      "      \"type\": \"function\",\n",
      "      \"description\": \"Read a file with the given path.\"\n",
      "    }\n",
      "  ],\n",
      "  \"top_p\": 1.0,\n",
      "  \"background\": false,\n",
      "  \"conversation\": null,\n",
      "  \"max_output_tokens\": null,\n",
      "  \"max_tool_calls\": null,\n",
      "  \"previous_response_id\": null,\n",
      "  \"prompt\": null,\n",
      "  \"prompt_cache_key\": null,\n",
      "  \"reasoning\": {\n",
      "    \"effort\": \"medium\",\n",
      "    \"generate_summary\": null,\n",
      "    \"summary\": null\n",
      "  },\n",
      "  \"safety_identifier\": null,\n",
      "  \"service_tier\": \"default\",\n",
      "  \"status\": \"completed\",\n",
      "  \"text\": {\n",
      "    \"format\": {\n",
      "      \"type\": \"text\"\n",
      "    },\n",
      "    \"verbosity\": \"medium\"\n",
      "  },\n",
      "  \"top_logprobs\": 0,\n",
      "  \"truncation\": \"disabled\",\n",
      "  \"usage\": {\n",
      "    \"input_tokens\": 269,\n",
      "    \"input_tokens_details\": {\n",
      "      \"cached_tokens\": 0\n",
      "    },\n",
      "    \"output_tokens\": 18,\n",
      "    \"output_tokens_details\": {\n",
      "      \"reasoning_tokens\": 0\n",
      "    },\n",
      "    \"total_tokens\": 287\n",
      "  },\n",
      "  \"user\": null,\n",
      "  \"store\": true\n",
      "}\n",
      "Final output:\n",
      "\n",
      "Created file 'test-function-calling.txt' with the requested content.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# 1. Define a list of callable tools for the model\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"name\": \"write_file\",\n",
    "        \"description\": \"Write a file with the given content.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"file_path\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The path to the file to write to.\",\n",
    "                },\n",
    "                \"content\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The content to write to the file.\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"file_path\", \"content\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"name\": \"read_file\",\n",
    "        \"description\": \"Read a file with the given path.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"file_path\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The path to the file to read.\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"file_path\"],\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "def read_file(file_path: str) -> str:\n",
    "    \"\"\"Read UTF-8 text from a file under BASE_DIR.\"\"\"\n",
    "    p = _ensure_under_base(Path(file_path))\n",
    "    return p.read_text(encoding=\"utf-8\")\n",
    "\n",
    "def write_file(file_path: str, content: str) -> str:\n",
    "    \"\"\"Write UTF-8 text to a file under BASE_DIR (create parents).\"\"\"\n",
    "    p = _ensure_under_base(Path(file_path))\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    p.write_text(content, encoding=\"utf-8\")\n",
    "    return f\"Wrote {len(content)} chars to {p.relative_to(BASE_DIR)}\"\n",
    "\n",
    "\n",
    "# Create a running input list we will add to over time\n",
    "input_list = [\n",
    "    {\"role\": \"user\", \"content\": \"Create a file called 'test-function-calling.txt' with the content 'Level 6: Function calling!'\"}\n",
    "]\n",
    "\n",
    "# 2. Prompt the model with tools defined\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\", # setting the choice of tool call to be automatically selected by the model\n",
    "    input=input_list,\n",
    ")\n",
    "\n",
    "# Save function call outputs for subsequent requests\n",
    "input_list += response.output\n",
    "\n",
    "for item in response.output:\n",
    "    if item.type == \"function_call\":\n",
    "        if item.name == \"write_file\":\n",
    "            # 3. Execute the function logic\n",
    "            func_args = json.loads(item.arguments)\n",
    "            file_path = func_args[\"file_path\"]\n",
    "            content = func_args[\"content\"]\n",
    "            output_write_file = write_file(file_path, content)\n",
    "            # 4. Provide function call results to the model\n",
    "            input_list.append({\n",
    "                \"type\": \"function_call_output\",\n",
    "                \"call_id\": item.call_id,\n",
    "                \"output\": json.dumps({\n",
    "                  \"output_content\": output_write_file\n",
    "                })\n",
    "            })\n",
    "        elif item.name == \"read_file\":\n",
    "            # 3. Execute the function logic\n",
    "            # Here we can feed the item.arguments directly because it's only one argument\n",
    "            output_read_file = read_file(json.loads(item.arguments))\n",
    "            # 4. Provide function call results to the model\n",
    "            input_list.append({\n",
    "                \"type\": \"function_call_output\",\n",
    "                \"call_id\": item.call_id,\n",
    "                \"output\": json.dumps({\n",
    "                  \"output_content\": output_read_file\n",
    "                })\n",
    "            })\n",
    "\n",
    "print(\"Final input:\")\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5-mini\",\n",
    "    instructions=\"Answer the user's question leveraging the information obtained from the usage of the tools.\",\n",
    "    tools=tools,\n",
    "    input=input_list,\n",
    ")\n",
    "\n",
    "# 5. The model should be able to give a response!\n",
    "output_json = response.model_dump_json(indent=2)\n",
    "print(output_json)\n",
    "print(\"Final output:\")\n",
    "print(\"\\n\" + response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2c5adee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 6: Function calling!"
     ]
    }
   ],
   "source": [
    "!cat test-function-calling.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37ed5c9",
   "metadata": {},
   "source": [
    "# Level 7 - React style Agent Loop\n",
    "\n",
    "- Paper: https://arxiv.org/pdf/2210.03629\n",
    "\n",
    "<img src=\"./2025-08-30-00-06-29.png\" width=60%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0d912086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FINAL ---\n",
      "- Read sample_input.txt and generated a concise 2-sentence summary.\n",
      "- Wrote the summary to summary_file.txt.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# ---------- Python-side tools ----------\n",
    "def read_file(file_path: str) -> str:\n",
    "    p = Path(file_path)\n",
    "    if not p.exists():\n",
    "        return f\"Error: file not found: {file_path}\"\n",
    "    try:\n",
    "        return p.read_text(encoding=\"utf-8\")\n",
    "    except Exception as e:\n",
    "        return f\"Error reading '{file_path}': {e}\"\n",
    "\n",
    "def write_file(file_path: str, content: str) -> str:\n",
    "    p = Path(file_path)\n",
    "    try:\n",
    "        p.parent.mkdir(parents=True, exist_ok=True)\n",
    "        p.write_text(content, encoding=\"utf-8\")\n",
    "        return f\"Wrote {len(content)} chars to {file_path}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error writing '{file_path}': {e}\"\n",
    "\n",
    "PY_TOOLS: Dict[str, Callable[..., str]] = {\n",
    "    \"read_file\": read_file,\n",
    "    \"write_file\": write_file,\n",
    "}\n",
    "\n",
    "# ---------- Responses API tool schema ----------\n",
    "TOOLS = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"name\": \"read_file\",\n",
    "        \"description\": \"Read text content from a UTF-8 file relative to the working directory.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"file_path\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Path to the file to read (e.g., 'data/input.txt').\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"file_path\"],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"name\": \"write_file\",\n",
    "        \"description\": \"Write UTF-8 text content to a file (creates directories if needed).\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"file_path\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Where to write (e.g., 'out/summary.txt').\"\n",
    "                },\n",
    "                \"content\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Text to write into the file.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"file_path\", \"content\"],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "instructions_prompt = \"\"\"\n",
    "    You are a careful assistant that can use tools to read and write files.\n",
    "    - If a task involves files, call the appropriate tool with precise arguments.\n",
    "    - After tool calls, summarize results for the user.\n",
    "    - Only write files when explicitly asked or when necessary to complete the task.\n",
    "    - If something is ambiguous (e.g., missing path), ask a concise clarifying question.\n",
    "    When you are done, respond starting with 'Final Answer:' and keep it concise.\n",
    "\"\"\"\n",
    "\n",
    "def safe_load_json(s: str):\n",
    "    try:\n",
    "        return json.loads(s or \"{}\")\n",
    "    except json.JSONDecodeError:\n",
    "        s2 = (s or \"\").replace(\"'\", '\"').rstrip(\",\")\n",
    "        try:\n",
    "            return json.loads(s2)\n",
    "        except Exception:\n",
    "            return {}\n",
    "\n",
    "def summarize_for_model(text: str, limit: int = 2000) -> str:\n",
    "    t = str(text)\n",
    "    return t if len(t) <= limit else (t[:limit] + f\"... [truncated {len(t)-limit} chars]\")\n",
    "\n",
    "def level7_react_agent_loop(task_prompt: str, *, max_turns: int = 8) -> str:\n",
    "    \"\"\"\n",
    "    ReAct-style loop using OpenAI Responses API with proper function_call / function_call_output items.\n",
    "    \"\"\"\n",
    "    input_list = [\n",
    "        {\"role\": \"user\", \"content\": task_prompt}\n",
    "    ]\n",
    "    tool_only_streak = 0\n",
    "\n",
    "    for _ in range(max_turns):\n",
    "        # 1) Ask the model with current transcript (items) + tools\n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-5\",\n",
    "            tools=TOOLS,\n",
    "            input=input_list,\n",
    "            instructions=instructions_prompt,\n",
    "        )\n",
    "\n",
    "        # 2) Append all model output items to running input_list\n",
    "        input_list += response.output\n",
    "\n",
    "        # 3) Find any function calls in this turn\n",
    "        fn_calls = [it for it in response.output if it.type == \"function_call\"]\n",
    "\n",
    "        if fn_calls:\n",
    "            tool_only_streak += 1\n",
    "            # 4) Execute each function call and append function_call_output items\n",
    "            for call in fn_calls:\n",
    "                name = call.name\n",
    "                args = safe_load_json(call.arguments)\n",
    "                fn = PY_TOOLS.get(name)\n",
    "                if not fn:\n",
    "                    result = json.dumps({\"error\": f\"unknown tool '{name}'\", \"available\": list(PY_TOOLS)})\n",
    "                else:\n",
    "                    try:\n",
    "                        result_text = fn(**args)\n",
    "                    except TypeError as e:\n",
    "                        result_text = f\"TypeError for '{name}' with args {args}: {e}\"\n",
    "                    except Exception as e:\n",
    "                        result_text = f\"Tool '{name}' failed: {e}\"\n",
    "                    result = json.dumps({\"result\": summarize_for_model(result_text)})\n",
    "\n",
    "                input_list.append({\n",
    "                    \"type\": \"function_call_output\",\n",
    "                    \"call_id\": call.call_id,\n",
    "                    \"output\": result,\n",
    "                })\n",
    "\n",
    "            # 5) If the model is stuck in tool-only mode, nudge to conclude\n",
    "            if tool_only_streak > 6:\n",
    "                input_list.append({\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"Please conclude with 'Final Answer:' now.\"\n",
    "                })\n",
    "            # Continue loop so the model can see the outputs we just appended\n",
    "            continue\n",
    "\n",
    "        # 6) No tool calls — try to return text\n",
    "        tool_only_streak = 0\n",
    "        text = (response.output_text or \"\").strip()\n",
    "        if text:\n",
    "            m = re.search(r\"Final Answer:\\s*(.*)\", text, flags=re.I | re.S)\n",
    "            return (m.group(1).strip() if m else text)\n",
    "\n",
    "        # 7) Fallback nudge if we got non-text, non-function output\n",
    "        input_list.append({\"role\": \"user\", \"content\": \"Please provide a clear answer or call a tool.\"})\n",
    "\n",
    "    return \"Stopped: max_turns reached without a final answer.\"\n",
    "\n",
    "# ---------- Demo ----------\n",
    "\n",
    "user_task = (\n",
    "    \"Read 'sample_input.txt' and write a concise 2-sentence summary \"\n",
    "    \"to 'summary_file.txt'. Then confirm what you did.\"\n",
    ")\n",
    "print(\"\\n--- FINAL ---\\n\" + level7_react_agent_loop(user_task))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eee0603",
   "metadata": {},
   "source": [
    "# Level 8 - Agent as a Step of an Agentic Workflow\n",
    "\n",
    "- [Building Effective Agents by Anthropic](https://www.anthropic.com/engineering/building-effective-agents)\n",
    "- [Agents vs Workflows: Why Not Both? — Sam Bhagwat, Mastra.ai](https://youtu.be/8SUJEqQNClw?t=708)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78afee57",
   "metadata": {},
   "source": [
    "<img src=\"2025-08-30-02-48-11.png\" width=60%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a558b53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joke\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "class RouterOutput(BaseModel):\n",
    "    category: Literal[\"joke\",\"websearch\"] = Field(description=\"The category of the input (joke or websearch).\")\n",
    "\n",
    "output = client.responses.parse(\n",
    "    model=\"gpt-5-mini\",\n",
    "    input=\"I want to hear a joke\",\n",
    "    instructions=\"You are a helpful assistant that can tell jokes.\",\n",
    "    text_format=RouterOutput\n",
    ")\n",
    "\n",
    "print(output.output_parsed.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b31ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "JOKER_AGENT_TOOLS = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"name\": \"write_file\",\n",
    "        \"description\": \"Write UTF-8 text content to a file (creates directories if needed).\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"file_path\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Where to write (e.g., 'out/summary.txt').\"\n",
    "                },\n",
    "                \"content\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Text to write into the file.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"file_path\", \"content\"],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "WEBSEARCH_AGENT_TOOLS = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"name\": \"write_file\",\n",
    "        \"description\": \"Write UTF-8 text content to a file (creates directories if needed).\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"file_path\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Where to write (e.g., 'out/summary.txt').\"\n",
    "                },\n",
    "                \"content\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Text to write into the file.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"file_path\", \"content\"],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "    },\n",
    "    {\"type\": \"web_search\"}\n",
    "]\n",
    "\n",
    "agent_routes = {\n",
    "    \"joke\": {\n",
    "        \"name\": \"Joker Agent\",\n",
    "        \"instructions\": \"You are an expert joke teller. Your output is always a super funny joke.\",\n",
    "        \"tools\": JOKER_AGENT_TOOLS\n",
    "    },\n",
    "    \n",
    "    \"websearch\": {\n",
    "        \"name\": \"Web Search Agent\",\n",
    "        \"instructions\": \"You are an expert web searcher. You use the web search tool to search \\\n",
    "            the web for the relevant information, given the user's input and then you output that\\\n",
    "            into an organized markdown style report saving it with the write_file tool.\",\n",
    "        \"tools\": WEBSEARCH_AGENT_TOOLS\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "cf26052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The web search tool is called automatically by the OpenAI's responses\n",
    "tools_map = {\n",
    "    \"write_file\": write_file,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "2b71b85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def react_agent_loop(input: str, agent: Dict[str, str], *, max_turns: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    ReAct-style loop using OpenAI Responses API with proper function_call / function_call_output items.\n",
    "    \"\"\"\n",
    "    input_list = [\n",
    "        {\"role\": \"user\", \"content\": input}\n",
    "    ]\n",
    "    tool_only_streak = 0\n",
    "    AGENT_TOOLS = agent[\"tools\"]\n",
    "\n",
    "    for i in range(max_turns):\n",
    "        print(\"--------------------------------\")\n",
    "        print(f\"Iteration: {i}\")\n",
    "        # 1) Ask the model with current transcript (items) + tools\n",
    "        response = client.responses.create(\n",
    "            model=\"gpt-5-mini\",\n",
    "            tools=AGENT_TOOLS,\n",
    "            input=input_list,\n",
    "            instructions=agent[\"instructions\"],\n",
    "        )\n",
    "        \n",
    "\n",
    "        # 2) Append all model output items to running input_list\n",
    "        input_list += response.output\n",
    "\n",
    "        # 3) Find any function calls in this turn\n",
    "        fn_calls = [it for it in response.output if it.type == \"function_call\"]\n",
    "\n",
    "        if fn_calls:\n",
    "            tool_only_streak += 1\n",
    "            # 4) Execute each function call and append function_call_output items\n",
    "            for call in fn_calls:\n",
    "                name = call.name\n",
    "                args = safe_load_json(call.arguments)\n",
    "                fn = tools_map.get(name)\n",
    "                if not fn:\n",
    "                    result = json.dumps({\"error\": f\"unknown tool '{name}'\", \"available\": AGENT_TOOLS})\n",
    "                else:\n",
    "                    try:\n",
    "                        result_text = fn(**args)\n",
    "                    except TypeError as e:\n",
    "                        result_text = f\"TypeError for '{name}' with args {args}: {e}\"\n",
    "                    except Exception as e:\n",
    "                        result_text = f\"Tool '{name}' failed: {e}\"\n",
    "                    result = json.dumps({\"result\": summarize_for_model(result_text)})\n",
    "\n",
    "                input_list.append({\n",
    "                    \"type\": \"function_call_output\",\n",
    "                    \"call_id\": call.call_id,\n",
    "                    \"output\": result,\n",
    "                })\n",
    "\n",
    "            # 5) If the model is stuck in tool-only mode, nudge to conclude\n",
    "            if tool_only_streak > 6:\n",
    "                input_list.append({\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"Please conclude with 'Final Answer:' now.\"\n",
    "                })\n",
    "            # Continue loop so the model can see the outputs we just appended\n",
    "            continue\n",
    "\n",
    "        # 6) No tool calls — try to return text\n",
    "        tool_only_streak = 0\n",
    "        text = (response.output_text or \"\").strip()\n",
    "        if text:\n",
    "            m = re.search(r\"Final Answer:\\s*(.*)\", text, flags=re.I | re.S)\n",
    "            return (m.group(1).strip() if m else text)\n",
    "\n",
    "        # 7) Fallback nudge if we got non-text, non-function output\n",
    "        input_list.append({\"role\": \"user\", \"content\": \"Please provide a clear answer or call a tool.\"})\n",
    "\n",
    "    return \"Stopped: max_turns reached without a final answer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "fe7a75c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def level8_agent_as_step_of_agentic_workflow(input: str, agent_routes: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Route the input to the appropriate route based on the input.\n",
    "    \"\"\"\n",
    "    \n",
    "    output = client.responses.parse(\n",
    "        model=\"gpt-5-mini\",\n",
    "        input=input,\n",
    "        instructions=\"You are a helpful assistant that can tell jokes.\",\n",
    "        text_format=RouterOutput)\n",
    "    print(f\"Router output: {output.output_parsed.category}\")\n",
    "    selected_agent = agent_routes[output.output_parsed.category]\n",
    "    print(f\"Selected agent: {selected_agent['name']}\")\n",
    "    print(\"-\" * 100)\n",
    "    return react_agent_loop(input, selected_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f42ca8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Router output: joke\n",
      "Selected agent: Joker Agent\n",
      "----------------------------------------------------------------------------------------------------\n",
      "--------------------------------\n",
      "Iteration: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Why did the AI go to art school? To learn how to draw better conclusions.'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_joke = \"Tell me a joke about artificial intelligence.\"\n",
    "level8_agent_as_step_of_agentic_workflow(input_joke, agent_routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5e08d91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Router output: joke\n",
      "Selected agent: Joker Agent\n",
      "----------------------------------------------------------------------------------------------------\n",
      "--------------------------------\n",
      "Iteration: 0\n",
      "--------------------------------\n",
      "Iteration: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Saved to ai-jokes.md.\\n\\nHere are the 5 jokes about overhyping AI:\\n\\n1. They said AI would replace my job, so I taught it to make coffee. Now it runs a hugely successful TikTok about latte art while I keep getting promoted for actually doing the work.\\n\\n2. The hype said AI would make everything smarter. Turns out my toaster only got smarter at asking for a software update — every time it burns the toast it blames \"legacy carbs.\"\\n\\n3. Investors poured money into an AI that \"understands human emotion.\" It nodded thoughtfully at meetings, charged premium subscription fees, and ghosted everyone when the project went dark.\\n\\n4. I asked an AI for life advice and it replied, \"According to my analysis, you should buy Bitcoin and start a podcast.\" I asked it for dinner ideas and it suggested starting a podcast about Bitcoin instead.\\n\\n5. The company branded its chatbot as \"groundbreaking\" and \"revolutionary.\" It was mainly used to forward office memes and pretend to be in meetings. Revolution achieved.'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_joke = \"Write 5 funny jokes about over hyping AI and save them to a file called 'ai-jokes.md'\"\n",
    "level8_agent_as_step_of_agentic_workflow(input_joke, agent_routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ee0bb2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Router output: websearch\n",
      "Selected agent: Web Search Agent\n",
      "----------------------------------------------------------------------------------------------------\n",
      "--------------------------------\n",
      "Iteration: 0\n",
      "--------------------------------\n",
      "Iteration: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I've saved the summary to agentic_workflows_2025_summary.md (on the assistant's file system). The file contains a concise 2025-focused literature review, key papers, trends, open challenges, and recommendations, with citations to the sources I found.\\n\\nWould you like me to:\\n- Expand any paper's entry into a longer annotated note with key figures/method details?\\n- Fetch and attach the PDFs or GitHub repos for selected papers (AFlow, MermaidFlow, etc.)?\\n- Produce slides or a short presentation summarizing the findings for a team?\""
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_websearch = \"\"\"\n",
    "Research the latest papers from 2025 on building agentic \n",
    "workflows and write me a summary of what you found to a \n",
    "file called 'agentic_workflows_2025_summary.md'\n",
    "\"\"\"\n",
    "level8_agent_as_step_of_agentic_workflow(input_websearch, agent_routes)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
